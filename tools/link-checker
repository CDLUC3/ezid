#! /usr/bin/env python

# Link checker that tests EZID target URLs.  Only non-default target
# URLs of public, real identifiers are tested.
#
# This script runs continuously and indefinitely.  It runs
# independently of the main EZID server, and may even run on a
# different machine, but is nevertheless loosely coupled to EZID in
# two ways.  1) It communicates with EZID through EZID's database
# (search database currently; store database in the future).
# Specifically, the link checker maintains its own table of
# identifiers and target URLs which it periodically updates from the
# main EZID tables, and conversely, the EZID server periodically
# uploads link checker results back into its tables.  These update
# mechanisms are asynchronous from mainline EZID processing.  2) The
# link checker lives in and uses some of EZID's codebase, principally
# to enable database access.
#
# The link checker tests a target URL by performing a GET request on
# the URL.  A timely 200 response equates to success.
#
# Between periodic (say, weekly) table updates the link checker
# processes limited-size worksets.  A workset consists of the "oldest"
# target URLs (those that were last checked longest ago) from each
# owner, up to a maximum number per owner.  Parallel worker threads
# then visit the URLs in round-robin fashion (i.e., visit one URL from
# each owner, then repeat the cycle) so as to dilute the burden the
# link checker places on external servers.  Additionally, the link
# checker imposes a minimum delay between successive checks against
# the same owner.  (There is typically a high correlation between
# owners and servers.)
#
# Blackout windows are an important feature.  Target URLs are not
# re-checked within a certain window of time (say, one month).
# Combined with the round-robin processing described above, the
# intention is to balance timeliness and exhaustivity (all target URLs
# will eventually be checked) and fairness (the checks of any given
# owner's target URLs will not be excessively delayed because another
# owner has many more identifiers than it).  Additionally, previously
# failed target URLs utilize a different window (say, 1-2 days) and
# are given priority in populating worksets, to allow failures to be
# re-checked more frequently.
#
# Failures are not reported immediately because transient outages are
# frequently encountered.  Only after a target URL consecutively fails
# some number of checks (say, a dozen over a span of two weeks) is it
# considered notification-worthy.
#
# Greg Janee <gjanee@ucop.edu>
# September 2016

import cookielib
import logging
import os
import threading
import time
import urllib2

# The following must precede any EZID module imports:
execfile(os.path.join(os.path.split(os.path.abspath(__file__))[0],
  "offline.py"))

import config
import ezidapp.models

TABLE_UPDATE_CYCLE = int(config.get("linkchecker.table_update_cycle"))
SUCCESS_RECHECK_MINIMUM = int(config.get(
  "linkchecker.success_recheck_minimum"))
FAILURE_RECHECK_MINIMUM = int(config.get(
  "linkchecker.failure_recheck_minimum"))
SAME_OWNER_MINIMUM = int(config.get("linkchecker.same_owner_minimum"))
NUM_WORKERS = int(config.get("linkchecker.num_workers"))
WORKSET_SIZE = int(config.get("linkchecker.workset_size"))
CHECK_TIMEOUT = int(config.get("linkchecker.check_timeout"))
USER_AGENT = config.get("linkchecker.user_agent")

_log = logging.getLogger()
_workset = None # { owner_id: [LinkChecker, ...] }
_stopNow = None
_stats = None # [total sleep time, total pass time, number of passes]
_statsLock = threading.Lock()

def s (n):
  if n != 1:
    return "s"
  else:
    return ""

def toHms (seconds):
  h = seconds/3600
  seconds -= h*3600
  m = seconds/60
  s = seconds-m*60
  return "%02d:%02d:%02d" % (h, m, s)

def now ():
  return int(time.time())

def remaining (start, cycle):
  return max(cycle-(now()-start), 0)

def harvest (model, only=None, filter=None):
  lastIdentifier = ""
  while True:
    qs = model.objects.filter(identifier__gt=lastIdentifier).order_by(
      "identifier")
    if only != None: qs = qs.only(*only)
    qs = list(qs[:1000])
    if len(qs) == 0: break
    for o in qs:
      if filter == None or filter(o): yield o
    lastIdentifier = qs[-1].identifier
  yield None

def updateDatabaseTable ():
  _log.info("begin update table")
  numIdentifiers = 0
  numAdditions = 0
  numDeletions = 0
  numUpdates = 0
  lcGenerator = harvest(ezidapp.models.LinkChecker)
  siGenerator = harvest(ezidapp.models.SearchIdentifier, ["identifier",
    "owner", "status", "target", "isTest"],
    lambda si: si.isPublic and not si.isTest and si.target != si.defaultTarget)
  lc = lcGenerator.next()
  si = siGenerator.next()
  while lc != None or si != None:
    if lc != None and (si == None or lc.identifier < si.identifier):
      numDeletions += 1
      lc.delete()
      lc = lcGenerator.next()
    elif si != None and (lc == None or si.identifier < lc.identifier):
      numIdentifiers += 1
      numAdditions += 1
      nlc = ezidapp.models.LinkChecker(identifier=si.identifier,
        target=si.target, owner_id=si.owner_id)
      nlc.full_clean(validate_unique=False)
      nlc.save()
      si = siGenerator.next()
    else:
      numIdentifiers += 1
      if lc.owner_id != si.owner_id or lc.target != si.target:
        numUpdates += 1
        lc.owner_id = si.owner_id
        lc.target = si.target
        lc.clearHistory()
        lc.full_clean(validate_unique=False)
        lc.save()
      lc = lcGenerator.next()
      si = siGenerator.next()
  _log.info(("end update table, %d identifier%s, %d addition%s, " +\
    "%d deletion%s, %d update%s") % (numIdentifiers, s(numIdentifiers),
    numAdditions, s(numAdditions), numDeletions, s(numDeletions),
    numUpdates, s(numUpdates)))

def loadWorkset ():
  global _workset
  _workset = {}
  _log.info("begin load workset")
  for user in ezidapp.models.SearchUser.objects.all().only("id"):
    def query (isBad, timeBound, limit):
      return list(ezidapp.models.LinkChecker.objects.filter(owner_id=user.id)\
        .filter(isBad=isBad).filter(lastCheckTime__lt=timeBound)\
        .order_by("lastCheckTime")[:limit])
    qs = query(True, now()-FAILURE_RECHECK_MINIMUM, WORKSET_SIZE)
    if WORKSET_SIZE-len(qs) > 0:
      qs.extend(query(False, now()-SUCCESS_RECHECK_MINIMUM,
        WORKSET_SIZE-len(qs)))
    if len(qs) > 0: _workset[user.id] = qs
  numOwners = len(_workset)
  numLinks = sum(len(l) for l in _workset.values())
  _log.info("end load workset, %d owner%s, %d link%s" % (numOwners,
    s(numOwners), numLinks, s(numLinks)))

def worker (workerIndex):
  global _stats
  try:
    indexes = dict((o, [workerIndex, 0]) for o in _workset.keys())
    # indexes = { owner_id: [current index, last check time] }
    while len(indexes) > 0:
      start = now()
      for owner_id, info in indexes.items():
        if _stopNow: return
        if info[0] >= len(_workset[owner_id]):
          del indexes[owner_id]
          continue
        t = now()-info[1]
        if t < SAME_OWNER_MINIMUM:
          time.sleep(SAME_OWNER_MINIMUM-t)
          _statsLock.acquire()
          try:
            _stats[0] += SAME_OWNER_MINIMUM-t
          finally:
            _statsLock.release()
        lc = _workset[owner_id][info[0]]
        # Some websites fall into infinite redirect loops if cookies
        # are not utilized.
        o = urllib2.build_opener(
          urllib2.HTTPCookieProcessor(cookielib.CookieJar()))
        r = urllib2.Request(lc.target, headers={ "User-Agent": USER_AGENT })
        c = None
        try:
          c = o.open(r, timeout=CHECK_TIMEOUT)
          content = c.read()
        except Exception, e:
          lc.checkFailed(e.code if isinstance(e, urllib2.HTTPError) else -1)
        else:
          lc.checkSucceeded(c.info().get("Content-Type", "unknown"), content)
        finally:
          if c: c.close()
        lc.full_clean(validate_unique=False)
        lc.save()
        info[0] += NUM_WORKERS
        info[1] = now()
      _statsLock.acquire()
      try:
        _stats[1] += now()-start
        _stats[2] += 1
      finally:
        _statsLock.release()
  except:
    _log.error("unhandled exception in worker thread %d" % workerIndex)
    raise

def main ():
  global _stats
  while True:
    start = now()
    updateDatabaseTable()
    # The following flag is used to ensure at least one round gets
    # fully processed.  In general rounds may be interrupted.
    firstRound = True
    while True:
      loadWorkset()
      _log.info("begin processing")
      if len(_workset) > 0:
        threads = []
        _stats = [0, 0, 0]
        for i in range(NUM_WORKERS):
          def wrapper (index):
            return lambda: worker(index)
          t = threading.Thread(target=wrapper(i))
          t.start()
          threads.append(t)
        for i in range(NUM_WORKERS):
          if firstRound:
            timeout = None
          else:
            timeout = remaining(start, TABLE_UPDATE_CYCLE)
          threads[i].join(timeout)
        _stopNow = True
        for i in range(NUM_WORKERS): threads[i].join()
        _log.info(("end processing, net effective average pass time %ss, " +\
          "total sleep time %s") % (str(round(float(_stats[1])/\
          (_stats[2] if _stats[2] > 0 else 1)/NUM_WORKERS, 1)),
          toHms(_stats[0])))
      else:
        # The sleep below is just to prevent a compute-intensive loop.
        _log.info("end processing (nothing to check)")
        time.sleep(60)
      if remaining(start, TABLE_UPDATE_CYCLE) == 0: break
      firstRound = False

if __name__ == "__main__": main()
