#! /usr/bin/env python

# Link checker that tests EZID target URLs.  Only non-default target
# URLs of public, real identifiers are tested.
#
# This script runs continuously and indefinitely.  It runs
# independently of the main EZID server, and may even run on a
# different machine, but is nevertheless loosely coupled to EZID in
# two ways.  1) It communicates with EZID through EZID's database
# (search database currently; store database in the future).
# Specifically, the link checker maintains its own table of
# identifiers and target URLs which it periodically updates from the
# main EZID tables, and conversely, the EZID server periodically
# uploads link checker results back into its tables.  These update
# mechanisms are asynchronous from mainline EZID processing.  2) The
# link checker lives in and uses some of EZID's codebase, principally
# to enable database access.
#
# The link checker tests a target URL by performing a GET request on
# the URL.  A timely 200 response equates to success.
#
# Between periodic (say, weekly) table updates the link checker
# processes limited-size worksets.  A workset consists of the "oldest"
# target URLs (those that were last checked longest ago) from each
# owner, up to a maximum number per owner.  Parallel worker threads
# then visit the URLs in round-robin fashion (i.e., visit one URL from
# each owner, then repeat the cycle) so as to dilute the burden the
# link checker places on external servers.  Additionally, the link
# checker imposes a minimum interval between successive checks against
# the same owner.  (There is typically a high correlation between
# owners and servers.)
#
# Blackout windows are an important feature.  Target URLs are not
# re-checked within a certain window of time (say, one month).
# Combined with the round-robin processing described above, the
# intention is to balance timeliness and exhaustivity (all target URLs
# will eventually be checked) and fairness (the checks of any given
# owner's target URLs will not be excessively delayed because another
# owner has many more identifiers than it).  Additionally, previously
# failed target URLs utilize a different window (say, 1-2 days) and
# are given priority in populating worksets, to allow failures to be
# re-checked more frequently.
#
# Failures are not reported immediately because transient outages are
# frequently encountered.  Only after a target URL consecutively fails
# some number of checks (say, a dozen over a span of two weeks) is it
# considered notification-worthy.
#
# Greg Janee <gjanee@ucop.edu>
# September 2016

import cookielib
import logging
import os
import threading
import time
import urllib2

# The following must precede any EZID module imports:
execfile(os.path.join(os.path.split(os.path.abspath(__file__))[0],
  "offline.py"))

import config
import ezidapp.models

TABLE_UPDATE_CYCLE = int(config.get("linkchecker.table_update_cycle"))
GOOD_RECHECK_MIN_INTERVAL = int(config.get(
  "linkchecker.good_recheck_min_interval"))
BAD_RECHECK_MIN_INTERVAL = int(config.get(
  "linkchecker.bad_recheck_min_interval"))
OWNER_REVISIT_MIN_INTERVAL = int(config.get(
  "linkchecker.owner_revisit_min_interval"))
NUM_WORKERS = int(config.get("linkchecker.num_workers"))
WORKSET_OWNER_MAX_LINKS = int(config.get(
  "linkchecker.workset_owner_max_links"))
CHECK_TIMEOUT = int(config.get("linkchecker.check_timeout"))
USER_AGENT = config.get("linkchecker.user_agent")
MAX_IO_FAILURES = 10

class OwnerWorkset (object):
  # Stores primarily a list of links to check that belong to a single
  # owner.  'nextIndex' points to the next unchecked link in the list;
  # if equal to the list length, all links have been checked.  While a
  # link is being checked, 'isLocked' is set to True.  'lastCheckTime'
  # is the last time a link from this owner was checked.
  def __init__ (self, owner_id, workList):
    self.owner_id = owner_id
    self.list = workList # [LinkChecker, ...]
    self.nextIndex = 0
    self.isLocked = False
    self.lastCheckTime = 0.0
    self.skip = False
  def isFinished (self):
    return self.skip or self.nextIndex >= len(self.list)

_log = logging.getLogger()
_lock = threading.Lock()
_workset = None # [OwnerWorkset, ...]
_index = None # index into _workset
_stopNow = None
_totalSleepTime = None

def s (n):
  if n != 1:
    return "s"
  else:
    return ""

def toHms (seconds):
  h = seconds/3600
  seconds -= h*3600
  m = seconds/60
  s = seconds-m*60
  return "%02d:%02d:%02d" % (h, m, s)

def now ():
  return time.time()

def nowi ():
  return int(now())

def remaining (start, cycle):
  return max(cycle-(now()-start), 0.0)

def daysSince (when):
  return int((now()-when)/86400)

def harvest (model, only=None, filter=None):
  lastIdentifier = ""
  while True:
    qs = model.objects.filter(identifier__gt=lastIdentifier).order_by(
      "identifier")
    if only != None: qs = qs.only(*only)
    qs = list(qs[:1000])
    if len(qs) == 0: break
    for o in qs:
      if filter == None or filter(o): yield o
    lastIdentifier = qs[-1].identifier
  yield None

def updateDatabaseTable ():
  _log.info("begin update table")
  numIdentifiers = 0
  numAdditions = 0
  numDeletions = 0
  numUpdates = 0
  numUnvisited = 0
  good = [0, 0, nowi()] # [total, to visit, oldest timestamp]
  bad = [0, 0, nowi()]
  lcGenerator = harvest(ezidapp.models.LinkChecker)
  siGenerator = harvest(ezidapp.models.SearchIdentifier, ["identifier",
    "owner", "status", "target", "isTest"],
    lambda si: si.isPublic and not si.isTest and si.target != si.defaultTarget)
  lc = lcGenerator.next()
  si = siGenerator.next()
  while lc != None or si != None:
    if lc != None and (si == None or lc.identifier < si.identifier):
      numDeletions += 1
      lc.delete()
      lc = lcGenerator.next()
    elif si != None and (lc == None or si.identifier < lc.identifier):
      numIdentifiers += 1
      numAdditions += 1
      numUnvisited += 1
      nlc = ezidapp.models.LinkChecker(identifier=si.identifier,
        target=si.target, owner_id=si.owner_id)
      nlc.full_clean(validate_unique=False)
      nlc.save()
      si = siGenerator.next()
    else:
      numIdentifiers += 1
      if lc.owner_id != si.owner_id or lc.target != si.target:
        numUpdates += 1
        numUnvisited += 1
        lc.owner_id = si.owner_id
        lc.target = si.target
        lc.clearHistory()
        lc.full_clean(validate_unique=False)
        lc.save()
      else:
        if lc.isUnvisited:
          numUnvisited += 1
        else:
          if lc.isGood:
            good[0] += 1
            if lc.lastCheckTime < nowi()-GOOD_RECHECK_MIN_INTERVAL:
              good[1] += 1
            good[2] = min(good[2], lc.lastCheckTime)
          else:
            bad[0] += 1
            if lc.lastCheckTime < nowi()-BAD_RECHECK_MIN_INTERVAL: bad[1] += 1
            bad[2] = min(bad[2], lc.lastCheckTime)
      lc = lcGenerator.next()
      si = siGenerator.next()
  _log.info(("end update table, %d identifier%s, %d addition%s, " +\
    "%d deletion%s, %d update%s, %d unvisited link%s, " +\
    "%d good link%s (%d to check, oldest=%dd), " +\
    "%d bad link%s (%d to check, oldest=%dd)") %\
    (numIdentifiers, s(numIdentifiers), numAdditions, s(numAdditions),
    numDeletions, s(numDeletions), numUpdates, s(numUpdates), numUnvisited,
    s(numUnvisited), good[0], s(good[0]), good[1], daysSince(good[2]),
    bad[0], s(bad[0]), bad[1], daysSince(bad[2])))

def loadWorkset ():
  global _workset
  _log.info("begin load workset")
  _workset = []
  numUnvisited = 0
  good = [0, nowi()] # [total, oldest timestamp]
  bad = [0, nowi()]
  for user in ezidapp.models.SearchUser.objects.all().only("id"):
    def query (isBad, timeBound, limit):
      return list(ezidapp.models.LinkChecker.objects.filter(owner_id=user.id)\
        .filter(isBad=isBad).filter(lastCheckTime__lt=timeBound)\
        .order_by("lastCheckTime")[:limit])
    qs = query(True, nowi()-BAD_RECHECK_MIN_INTERVAL, WORKSET_OWNER_MAX_LINKS)
    if len(qs) > 0:
      bad[0] += len(qs)
      bad[1] = min(bad[1], qs[0].lastCheckTime)
    if WORKSET_OWNER_MAX_LINKS-len(qs) > 0:
      q = query(False, nowi()-GOOD_RECHECK_MIN_INTERVAL,
        WORKSET_OWNER_MAX_LINKS-len(qs))
      if len(q) > 0:
        qs.extend(q)
        qgood = [lc for lc in q if lc.isVisited]
        numUnvisited += len(q)-len(qgood)
        if len(qgood) > 0:
          good[0] += len(qgood)
          good[1] = min(good[1], qgood[0].lastCheckTime)
    if len(qs) > 0: _workset.append(OwnerWorkset(user.id, qs))
  numOwners = len(_workset)
  numLinks = numUnvisited+good[0]+bad[0]
  _log.info(("end load workset, %d owner%s, %d link%s, " +\
    "%d unvisited link%s, %d good link%s (oldest=%dd), " +\
    "%d bad link%s (oldest=%dd)") %\
    (numOwners, s(numOwners), numLinks, s(numLinks), numUnvisited,
    s(numUnvisited), good[0], s(good[0]), daysSince(good[1]),
    bad[0], s(bad[0]), daysSince(bad[1])))

def getNextLink ():
  global _index
  _lock.acquire()
  try:
    startingIndex = _index
    allFinished = True
    t = now()
    while True:
      ow = _workset[_index]
      if not ow.isFinished():
        if not ow.isLocked and ow.lastCheckTime < t-OWNER_REVISIT_MIN_INTERVAL:
          # If an owner's links are consistently returning I/O or
          # other connection-related failures we mark the owner as
          # finished for now.  The remaining links will be picked up
          # in the next round, so all links will eventually get
          # checked, but this approach allows us to continue checking
          # other owners' links without wasting a lot of time waiting
          # on connection timeouts.
          if ow.nextIndex >= MAX_IO_FAILURES and\
            all(lc.returnCode == -1 for lc in ow.list[-MAX_IO_FAILURES:]):
            ow.skip = True
          else:
            ow.isLocked = True
            return (_index, ow.list[ow.nextIndex])
        else:
          allFinished = False
      _index = (_index+1)%len(_workset)
      if _index == startingIndex:
        return "finished" if allFinished else "wait"
  finally:
    _lock.release()

def markLinkChecked (index):
  _lock.acquire()
  try:
    ow = _workset[index]
    ow.nextIndex += 1
    ow.lastCheckTime = now()
    ow.isLocked = False
  finally:
    _lock.release()

def worker ():
  global _totalSleepTime
  try:
    while not _stopNow:
      r = getNextLink()
      if type(r) is str:
        if r == "finished":
          return
        else: # wait
          time.sleep(1)
          _lock.acquire()
          try:
            _totalSleepTime += 1
          finally:
            _lock.release()
          continue
      index, lc = r
      # Some websites fall into infinite redirect loops if cookies
      # are not utilized.
      o = urllib2.build_opener(
        urllib2.HTTPCookieProcessor(cookielib.CookieJar()))
      c = None
      try:
        c = o.open(urllib2.Request(lc.target,
          headers={ "User-Agent": USER_AGENT }), timeout=CHECK_TIMEOUT)
        content = c.read()
      except Exception, e:
        lc.checkFailed(e.code if isinstance(e, urllib2.HTTPError) else -1)
      else:
        lc.checkSucceeded(c.info().get("Content-Type", "unknown"), content)
      finally:
        if c: c.close()
      lc.full_clean(validate_unique=False)
      lc.save()
      markLinkChecked(index)
  except:
    _log.error("unhandled exception in worker thread")
    raise

def main ():
  global _stopNow, _index, _totalSleepTime
  while True:
    start = now()
    updateDatabaseTable()
    # The following flag is used to ensure at least one round gets
    # fully processed.  In general rounds may be interrupted.
    firstRound = True
    while firstRound or remaining(start, TABLE_UPDATE_CYCLE) > 0:
      loadWorkset()
      _log.info("begin processing")
      if len(_workset) > 0:
        _stopNow = False
        _index = 0
        _totalSleepTime = 0
        threads = []
        for i in range(NUM_WORKERS):
          t = threading.Thread(target=worker)
          t.start()
          threads.append(t)
        for i in range(NUM_WORKERS):
          if firstRound:
            timeout = None
          else:
            timeout = remaining(start, TABLE_UPDATE_CYCLE)
          threads[i].join(timeout)
          if threads[i].is_alive():
            # If the thread is still alive it must have timed out,
            # meaning it's time to terminate it and all remaining
            # threads.
            _stopNow = True
            threads[i].join()
        rate = sum(ow.nextIndex for ow in _workset)/(now()-start)
        if rate >= 1:
           rate = str(round(rate, 1)) + " links/s"
        else:
           rate = "1/%s link/s" % str(round(1/rate, 1))
        _log.info("end processing, checked %s, slept %s" %\
          (rate, toHms(_totalSleepTime)))
      else:
        # The sleep below is just to prevent a compute-intensive loop.
        _log.info("end processing (nothing to check)")
        time.sleep(60)
      firstRound = False

if __name__ == "__main__": main()
